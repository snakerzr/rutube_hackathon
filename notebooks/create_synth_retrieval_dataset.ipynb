{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание датасета и вопросов для RAG из файлов с сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../data/from_site/md\", glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs={'encoding':'utf-8'})\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"../data/from_site/md/faq\", glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs={'encoding':'utf-8'})\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    metadata = doc.metadata\n",
    "    page_content = doc.page_content\n",
    "    doc_chunks = markdown_splitter.split_text(page_content)\n",
    "    for chunk in doc_chunks:\n",
    "        chunk.metadata = chunk.metadata | metadata\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "def complete(query: str, instruction: str):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def extract_content(completion) -> str:\n",
    "    try:\n",
    "        return completion.choices[0].message.content\n",
    "    except:\n",
    "        return '==ERROR=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_from_document(doc) -> str:\n",
    "    headers = '\\n'.join([doc.metadata[meta] for meta in doc.metadata if meta.startswith('Header')])\n",
    "    page_content = doc.page_content\n",
    "    result = headers + '\\n' + page_content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация вопроса на 1 чанк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"Тебе прислали кусок текста из FAQ, придумай вопрос, ответ на который требовал бы информации из этого куска. Вопросы должны относиться с сервису RUTUBE. И быть написаны, будто бы они идут от клиента специалисту службы поддержки. Вопрос должен быть один.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1_chunk_instruction(chunk:str) -> str:\n",
    "    result = f'КУСОК ТЕКСТА ИЗ FAQ\\n=====\\n{chunk}\\n=====\\n'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers_single_chunk(documents_list:list[str]) -> list[dict]:\n",
    "    result = []\n",
    "    for doc in tqdm(documents_list):\n",
    "        sub_result = {}\n",
    "        chunk = create_1_chunk_instruction(create_chunk_from_document(doc))\n",
    "        question_completion = complete(chunk, SYSTEM_PROMPT)\n",
    "        question = extract_content(question_completion)\n",
    "        sub_result['question'] = question\n",
    "        sub_result['chunk'] = chunk\n",
    "        sub_result['metadata'] = {'answer': question_completion.to_dict()} | {'doc':doc.dict()}\n",
    "        result.append(sub_result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_answers_single_chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synth_complex_questions_1_chunk.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Данные успешно сохранены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация вопроса на 2 чанка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_pair(chunk_0:str, chunk_1:str) -> str:\n",
    "    result = f'''КУСОК 1:\\n=====\\n{chunk_0}\\n=====\\n\\nКУСОК 2\\n=====\\n{chunk_1}\\n=====\\n'''.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = \"Тебе прислали 2 куска текста, придумай вопрос, ответ на который требовал бы информации из двух чанков сразу. Вопросы должны относиться с сервису RUTUBE. И быть написаны, будто бы они идут от клиента специалисту службы поддержки.\"\n",
    "\n",
    "SYSTEM_PROMPT = \"Тебе прислали 2 куска текста, придумай вопрос, ответ на который требовал бы информации из двух чанков сразу. Вопросы должны относиться с сервису RUTUBE. И быть написаны, будто бы они идут от клиента специалисту службы поддержки. Вопрос должен быть один, чтобы ответ на него сочетал информацию из разных кусков.\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"Тебе прислали 2 куска текста, придумай вопрос, ответ на который требовал бы информации из двух чанков сразу. Вопросы должны относиться с сервису RUTUBE. И быть написаны, будто бы они идут от клиента специалисту службы поддержки. Вопрос должен быть один, чтобы ответ на него сочетал информацию из разных кусков. Вопрос должен быть сформулирован по обоим чанкам при условии одного от другого. Если вопрос куски текста по смыслу не подходят для создания объединенного вопроса, то напиши ПРОПУСК.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers_double_chunk(documents_list:list[tuple]) -> list[dict]:\n",
    "    result = []\n",
    "    for doc_a, doc_b in tqdm(documents_list):\n",
    "        sub_result = {}\n",
    "        chunk_a = create_chunk_from_document(doc_a)\n",
    "        chunk_b = create_chunk_from_document(doc_b)\n",
    "        chunk_pair = create_chunk_pair(chunk_a,chunk_b)\n",
    "        question_completion = complete(chunk_pair)\n",
    "        question = extract_content(question_completion)\n",
    "        sub_result['question'] = question\n",
    "        sub_result['chunk_a'] = chunk_a\n",
    "        sub_result['chunk_b'] = chunk_b\n",
    "        sub_result['metadata'] = {'answer': question_completion.to_dict()} | {'doc_a':doc_a.dict()} | {'doc_b': doc_b.dict()}\n",
    "        result.append(sub_result)\n",
    "\n",
    "    return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_pairs(lst, num_pairs):\n",
    "    all_pairs = [(a, b) for i, a in enumerate(lst) for b in lst[i+1:]]\n",
    "    \n",
    "    if num_pairs > len(all_pairs):\n",
    "        raise ValueError(\"Количество пар больше, чем возможные уникальные комбинации.\")\n",
    "    \n",
    "    return random.sample(all_pairs, num_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_answers_double_chunk(get_random_pairs(chunks, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synth_complex_questions_2_chunks.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Данные успешно сохранены\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
